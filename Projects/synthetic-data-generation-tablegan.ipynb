{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["# Synthetic Data Generation with GANS"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T03:39:59.725381Z","iopub.status.busy":"2022-06-10T03:39:59.724898Z","iopub.status.idle":"2022-06-10T03:40:01.115354Z","shell.execute_reply":"2022-06-10T03:40:01.114566Z","shell.execute_reply.started":"2022-06-10T03:39:59.725277Z"},"trusted":true},"outputs":[],"source":["import pandas as pd \n","import numpy as np\n","import seaborn as sns"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T03:40:01.119568Z","iopub.status.busy":"2022-06-10T03:40:01.118629Z","iopub.status.idle":"2022-06-10T03:40:01.246504Z","shell.execute_reply":"2022-06-10T03:40:01.245336Z","shell.execute_reply.started":"2022-06-10T03:40:01.119509Z"},"trusted":true},"outputs":[],"source":["df = pd.read_csv(\"../input/asteroid-impacts/orbits - orbits.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T03:40:01.249410Z","iopub.status.busy":"2022-06-10T03:40:01.248751Z","iopub.status.idle":"2022-06-10T03:40:01.886653Z","shell.execute_reply":"2022-06-10T03:40:01.885240Z","shell.execute_reply.started":"2022-06-10T03:40:01.249367Z"},"trusted":true},"outputs":[],"source":["sns.heatmap(df.isnull(),cbar=False,yticklabels=False,cmap = 'viridis')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T03:40:01.889657Z","iopub.status.busy":"2022-06-10T03:40:01.889314Z","iopub.status.idle":"2022-06-10T03:40:01.898016Z","shell.execute_reply":"2022-06-10T03:40:01.897135Z","shell.execute_reply.started":"2022-06-10T03:40:01.889595Z"},"trusted":true},"outputs":[],"source":["df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T03:40:01.899856Z","iopub.status.busy":"2022-06-10T03:40:01.899308Z","iopub.status.idle":"2022-06-10T03:40:01.996712Z","shell.execute_reply":"2022-06-10T03:40:01.995832Z","shell.execute_reply.started":"2022-06-10T03:40:01.899786Z"},"trusted":true},"outputs":[],"source":["df.replace(np.nan, inplace = True)\n","df=df.dropna()\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T03:40:01.998668Z","iopub.status.busy":"2022-06-10T03:40:01.998274Z","iopub.status.idle":"2022-06-10T03:40:02.021198Z","shell.execute_reply":"2022-06-10T03:40:02.018869Z","shell.execute_reply.started":"2022-06-10T03:40:01.998619Z"},"trusted":true},"outputs":[],"source":["y = df['Classification'].astype('category').cat.codes\n","features = ['Epoch (TDB)', 'Orbit Axis (AU)', 'Orbit Eccentricity',\n","       'Orbit Inclination (deg)', 'Perihelion Argument (deg)',\n","       'Node Longitude (deg)', 'Mean Anomoly (deg)',\n","       'Perihelion Distance (AU)', 'Aphelion Distance (AU)',\n","       'Orbital Period (yr)', 'Minimum Orbit Intersection Distance (AU)',\n","       'Orbital Reference', 'Asteroid Magnitude', 'Hazardous']\n","X = df[features]"]},{"cell_type":"markdown","metadata":{},"source":["**Permutation Importance**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T03:40:02.024015Z","iopub.status.busy":"2022-06-10T03:40:02.023163Z","iopub.status.idle":"2022-06-10T03:40:02.567524Z","shell.execute_reply":"2022-06-10T03:40:02.566334Z","shell.execute_reply.started":"2022-06-10T03:40:02.023960Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T03:40:02.570313Z","iopub.status.busy":"2022-06-10T03:40:02.569585Z","iopub.status.idle":"2022-06-10T03:40:02.582709Z","shell.execute_reply":"2022-06-10T03:40:02.580304Z","shell.execute_reply.started":"2022-06-10T03:40:02.570257Z"},"trusted":true},"outputs":[],"source":["df.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T03:40:02.585181Z","iopub.status.busy":"2022-06-10T03:40:02.584657Z","iopub.status.idle":"2022-06-10T03:40:02.800529Z","shell.execute_reply":"2022-06-10T03:40:02.799533Z","shell.execute_reply.started":"2022-06-10T03:40:02.585130Z"},"trusted":true},"outputs":[],"source":["y_perm = (df['Classification'])\n","feature_names = ['Epoch (TDB)', 'Orbit Axis (AU)', 'Orbit Eccentricity',\n","       'Orbit Inclination (deg)', 'Perihelion Argument (deg)',\n","       'Node Longitude (deg)', 'Mean Anomoly (deg)',\n","       'Perihelion Distance (AU)', 'Aphelion Distance (AU)',\n","       'Orbital Period (yr)', 'Minimum Orbit Intersection Distance (AU)',\n","       'Orbital Reference', 'Asteroid Magnitude', 'Hazardous']\n","X_perm = df[features]\n","train_X, val_X, train_y, val_y = train_test_split(X_perm, y_perm, train_size = 90,random_state=1)\n","my_model = RandomForestClassifier(n_estimators=100,\n","                                  random_state=0).fit(train_X, train_y)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T03:40:02.803736Z","iopub.status.busy":"2022-06-10T03:40:02.803369Z","iopub.status.idle":"2022-06-10T03:40:21.956089Z","shell.execute_reply":"2022-06-10T03:40:21.954871Z","shell.execute_reply.started":"2022-06-10T03:40:02.803690Z"},"trusted":true},"outputs":[],"source":["import eli5\n","from eli5.sklearn import PermutationImportance\n","\n","perm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)\n","eli5.show_weights(perm, feature_names = val_X.columns.tolist())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T03:41:37.882520Z","iopub.status.busy":"2022-06-10T03:41:37.881519Z","iopub.status.idle":"2022-06-10T03:41:37.955080Z","shell.execute_reply":"2022-06-10T03:41:37.953988Z","shell.execute_reply.started":"2022-06-10T03:41:37.882468Z"},"trusted":true},"outputs":[],"source":["df = pd.read_csv('../input/asteroid-impacts/orbits - orbits.csv')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T03:41:38.737896Z","iopub.status.busy":"2022-06-10T03:41:38.736675Z","iopub.status.idle":"2022-06-10T03:41:38.744859Z","shell.execute_reply":"2022-06-10T03:41:38.744165Z","shell.execute_reply.started":"2022-06-10T03:41:38.737829Z"},"trusted":true},"outputs":[],"source":["df.columns\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T03:41:39.342203Z","iopub.status.busy":"2022-06-10T03:41:39.341327Z","iopub.status.idle":"2022-06-10T03:41:39.353825Z","shell.execute_reply":"2022-06-10T03:41:39.352533Z","shell.execute_reply.started":"2022-06-10T03:41:39.342146Z"},"trusted":true},"outputs":[],"source":["df.drop(['Object Name'], axis=1, inplace=True)\n","print(df.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T03:41:40.636084Z","iopub.status.busy":"2022-06-10T03:41:40.635707Z","iopub.status.idle":"2022-06-10T03:41:40.682728Z","shell.execute_reply":"2022-06-10T03:41:40.681706Z","shell.execute_reply.started":"2022-06-10T03:41:40.636044Z"},"trusted":true},"outputs":[],"source":["df.replace(np.nan, inplace = True)\n","df=df.dropna()\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T03:41:41.611773Z","iopub.status.busy":"2022-06-10T03:41:41.610847Z","iopub.status.idle":"2022-06-10T03:41:41.656755Z","shell.execute_reply":"2022-06-10T03:41:41.655865Z","shell.execute_reply.started":"2022-06-10T03:41:41.611705Z"},"trusted":true},"outputs":[],"source":["df.replace(np.nan, inplace = True)\n","df=df.dropna()\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T03:41:47.256832Z","iopub.status.busy":"2022-06-10T03:41:47.256492Z","iopub.status.idle":"2022-06-10T03:41:47.265832Z","shell.execute_reply":"2022-06-10T03:41:47.265050Z","shell.execute_reply.started":"2022-06-10T03:41:47.256777Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["df.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T03:41:48.576142Z","iopub.status.busy":"2022-06-10T03:41:48.575782Z","iopub.status.idle":"2022-06-10T03:41:48.583912Z","shell.execute_reply":"2022-06-10T03:41:48.582769Z","shell.execute_reply.started":"2022-06-10T03:41:48.576102Z"},"trusted":true},"outputs":[],"source":["# data configuration\n","\n","\n","file_name = \"../input/asteroid-impacts/orbits - orbits.csv\"\n","columns_to_drop = ['Object Name']\n","categorical_features = ['Classification']\n","continuous_features = ['Epoch (TDB)', 'Orbit Eccentricity', 'Perihelion Argument (deg)',\n","       'Node Longitude (deg)', 'Mean Anomoly (deg)', 'Orbit Inclination (deg)',\n","       'Aphelion Distance (AU)','Asteroid Magnitude',\n","       'Orbital Period (yr)', 'Minimum Orbit Intersection Distance (AU)',\n","       'Orbital Reference' ]\n","col_group_by = 'Hazardous'\n","col1, col2 = 'Perihelion Distance (AU)', 'Orbit Axis (AU)'\n","#col_group_by = 'Hazardous'\n","#col1, col2 = 'Perihelion Distance (AU)',  'Orbit Axis (AU)'\n","\n","# training configuration\n","noise_dim = 256\n","dim = 128\n","batch_size = 16\n","\n","log_step = 100\n","epochs = 5000+1\n","learning_rate = 5e-4\n","models_dir = 'model'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T03:41:49.802714Z","iopub.status.busy":"2022-06-10T03:41:49.802253Z","iopub.status.idle":"2022-06-10T03:41:49.883222Z","shell.execute_reply":"2022-06-10T03:41:49.882134Z","shell.execute_reply.started":"2022-06-10T03:41:49.802663Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","\n","df = pd.read_csv(file_name)\n","df.drop(columns_to_drop, axis=1, inplace=True)\n","print(df.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T03:41:51.213546Z","iopub.status.busy":"2022-06-10T03:41:51.212986Z","iopub.status.idle":"2022-06-10T03:41:51.245911Z","shell.execute_reply":"2022-06-10T03:41:51.244702Z","shell.execute_reply.started":"2022-06-10T03:41:51.213489Z"},"trusted":true},"outputs":[],"source":["for column in categorical_features:\n","    df[column] = df[column].astype('category').cat.codes\n","\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T03:41:52.213565Z","iopub.status.busy":"2022-06-10T03:41:52.212681Z","iopub.status.idle":"2022-06-10T03:41:52.448843Z","shell.execute_reply":"2022-06-10T03:41:52.448020Z","shell.execute_reply.started":"2022-06-10T03:41:52.213498Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","\n","for column in continuous_features:\n","    min = df[column].min()\n","    max = df[column].max()\n","    feature_bins = pd.cut(df[column], bins=np.linspace(min, max, 21), labels=False)\n","    df.drop([column], axis=1, inplace=True)\n","    df = pd.concat([df, feature_bins], axis=1)\n","    print(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T03:41:53.260518Z","iopub.status.busy":"2022-06-10T03:41:53.260177Z","iopub.status.idle":"2022-06-10T03:41:53.306951Z","shell.execute_reply":"2022-06-10T03:41:53.306182Z","shell.execute_reply.started":"2022-06-10T03:41:53.260487Z"},"trusted":true},"outputs":[],"source":["df.replace(np.nan, inplace = True)\n","df=df.dropna()\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T03:41:54.161471Z","iopub.status.busy":"2022-06-10T03:41:54.160977Z","iopub.status.idle":"2022-06-10T03:41:54.547274Z","shell.execute_reply":"2022-06-10T03:41:54.546283Z","shell.execute_reply.started":"2022-06-10T03:41:54.161417Z"},"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import PowerTransformer\n","\n","\n","df[df.columns] = PowerTransformer(method='yeo-johnson', standardize=True, copy=True).fit_transform(df[df.columns])\n","\n","print(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T03:41:55.581460Z","iopub.status.busy":"2022-06-10T03:41:55.580538Z","iopub.status.idle":"2022-06-10T03:41:55.953511Z","shell.execute_reply":"2022-06-10T03:41:55.952306Z","shell.execute_reply.started":"2022-06-10T03:41:55.581407Z"},"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import PowerTransformer\n","\n","\n","pw= PowerTransformer(method='yeo-johnson', standardize=True, copy=True)\n","pwt=pw.fit_transform(df[df.columns])\n","\n","print(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T03:41:56.156015Z","iopub.status.busy":"2022-06-10T03:41:56.155627Z","iopub.status.idle":"2022-06-10T03:41:56.164288Z","shell.execute_reply":"2022-06-10T03:41:56.163297Z","shell.execute_reply.started":"2022-06-10T03:41:56.155972Z"},"trusted":true},"outputs":[],"source":["df[df.columns]=pwt"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T03:41:56.896038Z","iopub.status.busy":"2022-06-10T03:41:56.895641Z","iopub.status.idle":"2022-06-10T03:41:56.925503Z","shell.execute_reply":"2022-06-10T03:41:56.923909Z","shell.execute_reply.started":"2022-06-10T03:41:56.895998Z"},"trusted":true},"outputs":[],"source":["df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T03:41:57.491073Z","iopub.status.busy":"2022-06-10T03:41:57.490544Z","iopub.status.idle":"2022-06-10T03:41:57.495330Z","shell.execute_reply":"2022-06-10T03:41:57.494317Z","shell.execute_reply.started":"2022-06-10T03:41:57.491034Z"},"trusted":true},"outputs":[],"source":["# calculate wasserstein loss\n","#def wasserstein_loss(y_true, y_pred):\n"," #   return backend.mean(y_true * y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T03:54:23.981362Z","iopub.status.busy":"2022-06-10T03:54:23.981025Z","iopub.status.idle":"2022-06-10T03:54:23.987281Z","shell.execute_reply":"2022-06-10T03:54:23.986443Z","shell.execute_reply.started":"2022-06-10T03:54:23.981327Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras.optimizers import Adam, RMSprop"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T03:55:37.103955Z","iopub.status.busy":"2022-06-10T03:55:37.102751Z","iopub.status.idle":"2022-06-10T03:55:37.141291Z","shell.execute_reply":"2022-06-10T03:55:37.140337Z","shell.execute_reply.started":"2022-06-10T03:55:37.103895Z"},"trusted":true},"outputs":[],"source":["import os\n","import numpy as np\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import Input, Dense, Dropout, LeakyReLU\n","from tensorflow.keras import Model\n","\n","from tensorflow.keras.optimizers import Adam, RMSprop\n","\n","class GAN():\n","    \n","    def __init__(self, gan_args):\n","        [self.batch_size, lr, self.noise_dim,\n","         self.data_dim, layers_dim] = gan_args\n","\n","        self.generator = Generator(self.batch_size).\\\n","            build_model(input_shape=(self.noise_dim,), dim=layers_dim, data_dim=self.data_dim)\n","\n","        self.discriminator = Discriminator(self.batch_size).\\\n","            build_model(input_shape=(self.data_dim,), dim=layers_dim)\n","\n","        optimizer = RMSprop(lr, lr=0.00005)\n","\n","        # Build and compile the discriminator\n","        self.discriminator.compile(loss='binary_crossentropy',\n","                                   optimizer=optimizer,\n","                                   metrics=['accuracy'])\n","\n","        # The generator takes noise as input and generates imgs\n","        z = Input(shape=(self.noise_dim,))\n","        record = self.generator(z)\n","\n","        # For the combined model we will only train the generator\n","        self.discriminator.trainable = False\n","\n","        # The discriminator takes generated images as input and determines validity\n","        validity = self.discriminator(record)\n","\n","        # The combined model  (stacked generator and discriminator)\n","        # Trains the generator to fool the discriminator\n","        self.combined = Model(z, validity)\n","        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n","\n","    def get_data_batch(self, train, batch_size, seed=0):\n","        # # random sampling - some samples will have excessively low or high sampling, but easy to implement\n","        # np.random.seed(seed)\n","        # x = train.loc[ np.random.choice(train.index, batch_size) ].values\n","        # iterate through shuffled indices, so every sample gets covered evenly\n","\n","        start_i = (batch_size * seed) % len(train)\n","        stop_i = start_i + batch_size\n","        shuffle_seed = (batch_size * seed) // len(train)\n","        np.random.seed(shuffle_seed)\n","        train_ix = np.random.choice(list(train.index), replace=False, size=len(train))  # wasteful to shuffle every time\n","        train_ix = list(train_ix) + list(train_ix)  # duplicate to cover ranges past the end of the set\n","        x = train.loc[train_ix[start_i: stop_i]].values\n","        return np.reshape(x, (batch_size, -1))\n","        \n","    def train(self, data, train_arguments):\n","        [cache_prefix, epochs, sample_interval] = train_arguments\n","        \n","        data_cols = data.columns\n","\n","        # Adversarial ground truths\n","        valid = np.ones((self.batch_size, 1))\n","        fake = np.zeros((self.batch_size, 1))\n","\n","        for epoch in range(epochs):    \n","            # ---------------------\n","            #  Train Discriminator\n","            # ---------------------\n","            batch_data = self.get_data_batch(data, self.batch_size)\n","            noise = tf.random.normal((self.batch_size, self.noise_dim))\n","\n","            # Generate a batch of new images\n","            gen_data = self.generator.predict(noise)\n","    \n","            # Train the discriminator\n","            d_loss_real = self.discriminator.train_on_batch(batch_data, valid)\n","            d_loss_fake = self.discriminator.train_on_batch(gen_data, fake)\n","            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n","    \n","            # ---------------------\n","            #  Train Generator\n","            # ---------------------\n","            noise = tf.random.normal((self.batch_size, self.noise_dim))\n","            # Train the generator (to have the discriminator label samples as valid)\n","            g_loss = self.combined.train_on_batch(noise, valid)\n","    \n","            # Plot the progress\n","            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100 * d_loss[1], g_loss))\n","    \n","            # If at save interval => save generated events\n","            if epoch % sample_interval == 0:\n","                #Test here data generation step\n","                # save model checkpoints\n","                model_checkpoint_base_name = 'model/' + cache_prefix + '_{}_model_weights_step_{}.h5'\n","                self.generator.save_weights(model_checkpoint_base_name.format('generator', epoch))\n","                self.discriminator.save_weights(model_checkpoint_base_name.format('discriminator', epoch))\n","\n","                #Here is generating the data\n","                z = tf.random.normal((432, self.noise_dim))\n","                gen_data = self.generator(z)\n","                print('generated_data')\n","\n","    def save(self, path, name):\n","        assert os.path.isdir(path) == True, \\\n","            \"Please provide a valid path. Path must be a directory.\"\n","        model_path = os.path.join(path, name)\n","        self.generator.save_weights(model_path)  # Load the generator\n","        return\n","    \n","    def load(self, path):\n","        assert os.path.isdir(path) == True, \\\n","            \"Please provide a valid path. Path must be a directory.\"\n","        self.generator = Generator(self.batch_size)\n","        self.generator = self.generator.load_weights(path)\n","        return self.generator\n","    \n","class Generator():\n","    def __init__(self, batch_size):\n","        self.batch_size=batch_size\n","        \n","    def build_model(self, input_shape, dim, data_dim):\n","        input= Input(shape=input_shape, batch_size=self.batch_size)\n","        x = Dense(dim, activation='LeakyReLU')(input)\n","        x = Dense(dim * 2, activation='LeakyReLU')(x)\n","        x = Dense(dim * 4, activation='LeakyReLU')(x)\n","        x = Dense(data_dim)(x)\n","        return Model(inputs=input, outputs=x)\n","\n","class Discriminator():\n","    def __init__(self,batch_size):\n","        self.batch_size=batch_size\n","    \n","    def build_model(self, input_shape, dim):\n","        input = Input(shape=input_shape, batch_size=self.batch_size)\n","        x = Dense(dim * 4, activation='LeakyReLU')(input)\n","        x = Dropout(0.1)(x)\n","        x = Dense(dim * 2, activation='LeakyReLU')(x)\n","        x = Dropout(0.1)(x)\n","        x = Dense(dim, activation='LeakyReLU')(x)\n","        x = Dense(1, activation='sigmoid')(x)\n","        return Model(inputs=input, outputs=x)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T03:55:38.186508Z","iopub.status.busy":"2022-06-10T03:55:38.185608Z","iopub.status.idle":"2022-06-10T03:55:38.191190Z","shell.execute_reply":"2022-06-10T03:55:38.190074Z","shell.execute_reply.started":"2022-06-10T03:55:38.186455Z"},"trusted":true},"outputs":[],"source":["data_cols = df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T03:55:38.720747Z","iopub.status.busy":"2022-06-10T03:55:38.720393Z","iopub.status.idle":"2022-06-10T03:55:38.737890Z","shell.execute_reply":"2022-06-10T03:55:38.736912Z","shell.execute_reply.started":"2022-06-10T03:55:38.720706Z"},"trusted":true},"outputs":[],"source":["#Define the GAN and training parameters\n","df[data_cols] = df[data_cols]\n","\n","print(df.shape[1])\n","\n","gan_args = [batch_size, learning_rate, noise_dim, df.shape[1], dim]\n","train_args = ['', epochs, log_step]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T03:55:39.226758Z","iopub.status.busy":"2022-06-10T03:55:39.226047Z","iopub.status.idle":"2022-06-10T03:55:41.642062Z","shell.execute_reply":"2022-06-10T03:55:41.640477Z","shell.execute_reply.started":"2022-06-10T03:55:39.226698Z"},"trusted":true},"outputs":[],"source":["!mkdir model\n","!mkdir model/gan\n","!mkdir model/gan/saved"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-06-10T03:55:43.743924Z","iopub.status.idle":"2022-06-10T03:55:43.744268Z","shell.execute_reply":"2022-06-10T03:55:43.744106Z","shell.execute_reply.started":"2022-06-10T03:55:43.744087Z"},"trusted":true},"outputs":[],"source":["from keras import backend"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T03:55:46.450656Z","iopub.status.busy":"2022-06-10T03:55:46.450297Z","iopub.status.idle":"2022-06-10T04:05:03.048332Z","shell.execute_reply":"2022-06-10T04:05:03.047317Z","shell.execute_reply.started":"2022-06-10T03:55:46.450620Z"},"trusted":true},"outputs":[],"source":["model = GAN\n","\n","#Training the GAN model chosen: Vanilla GAN, CGAN, DCGAN, etc.\n","synthesizer = model(gan_args)\n","synthesizer.train(df, train_args)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T04:17:38.886978Z","iopub.status.busy":"2022-06-10T04:17:38.886300Z","iopub.status.idle":"2022-06-10T04:17:39.703691Z","shell.execute_reply":"2022-06-10T04:17:39.702702Z","shell.execute_reply.started":"2022-06-10T04:17:38.886932Z"},"trusted":true},"outputs":[],"source":["!mkdir model/gan\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T04:17:39.707135Z","iopub.status.busy":"2022-06-10T04:17:39.706702Z","iopub.status.idle":"2022-06-10T04:17:39.745543Z","shell.execute_reply":"2022-06-10T04:17:39.744708Z","shell.execute_reply.started":"2022-06-10T04:17:39.707079Z"},"trusted":true},"outputs":[],"source":["synthesizer.save('model/gan/saved', 'asteroid')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T04:17:39.795622Z","iopub.status.busy":"2022-06-10T04:17:39.795272Z","iopub.status.idle":"2022-06-10T04:17:39.808979Z","shell.execute_reply":"2022-06-10T04:17:39.808042Z","shell.execute_reply.started":"2022-06-10T04:17:39.795577Z"},"trusted":true},"outputs":[],"source":["synthesizer.generator.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T04:17:43.206183Z","iopub.status.busy":"2022-06-10T04:17:43.205535Z","iopub.status.idle":"2022-06-10T04:17:43.219228Z","shell.execute_reply":"2022-06-10T04:17:43.218168Z","shell.execute_reply.started":"2022-06-10T04:17:43.206126Z"},"trusted":true},"outputs":[],"source":["synthesizer.discriminator.summary()"]},{"cell_type":"markdown","metadata":{},"source":["Now, that we have trained the model let's see if the generated data is similar to the actual data.\n","\n","We plot the generated data for some of the model steps and see how the plot for the generated data changes as the networks learns the embedding more accurately."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T04:17:51.741410Z","iopub.status.busy":"2022-06-10T04:17:51.741053Z","iopub.status.idle":"2022-06-10T04:17:51.746857Z","shell.execute_reply":"2022-06-10T04:17:51.746101Z","shell.execute_reply.started":"2022-06-10T04:17:51.741372Z"},"trusted":true},"outputs":[],"source":["models = {'GAN': ['GAN', False, synthesizer.generator]}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T04:20:36.515848Z","iopub.status.busy":"2022-06-10T04:20:36.514506Z","iopub.status.idle":"2022-06-10T04:20:42.204655Z","shell.execute_reply":"2022-06-10T04:20:42.203939Z","shell.execute_reply.started":"2022-06-10T04:20:36.515780Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# Setup parameters visualization parameters\n","seed = 17\n","test_size = 492 # number of fraud cases\n","noise_dim = 256\n","col_group_by = 'Hazardous'\n","col1, col2 = 'Perihelion Distance (AU)',  'Orbit Axis (AU)'\n","\n","np.random.seed(seed)\n","z = np.random.normal(size=(test_size, noise_dim))\n","real = synthesizer.get_data_batch(train=df, batch_size=test_size, seed=seed)\n","real_samples = pd.DataFrame(real, columns=data_cols)\n","\n","model_names = ['GAN']\n","colors = ['deepskyblue','blue']\n","markers = ['o','^']\n","\n","base_dir = 'model/'\n","\n","#Actual fraud data visualization\n","model_steps = [ 0, 100, 200, 300, 400, 500, 1000, 2000, 3000, 4000, 5000]\n","rows = len(model_steps)\n","columns = 5\n","\n","axarr = [[]]*len(model_steps)\n","\n","fig = plt.figure(figsize=(14,rows*3))\n","\n","for model_step_ix, model_step in enumerate(model_steps):        \n","    axarr[model_step_ix] = plt.subplot(rows, columns, model_step_ix*columns + 1)\n","    \n","    for group, color, marker in zip(real_samples.groupby(col_group_by), colors, markers):\n","        plt.scatter( group[1][[col1]], group[1][[col2]], marker=marker, edgecolors=color, facecolors='none' )\n","    \n","    plt.title('ACTUAL ORBITS DATA')\n","    plt.ylabel(col2) # Only add y label to left plot\n","    plt.xlabel(col1)\n","    xlims, ylims = axarr[model_step_ix].get_xlim(), axarr[model_step_ix].get_ylim()\n","    \n","    if model_step_ix == 0: \n","        legend = plt.legend()\n","        legend.get_frame().set_facecolor('white')\n","    \n","    i=0\n","    [model_name, with_class, generator_model] = models['GAN']\n","\n","    generator_model.load_weights( base_dir + '_generator_model_weights_step_'+str(model_step)+'.h5')\n","\n","    ax = plt.subplot(rows, columns, model_step_ix*columns + 1 + (i+1) )\n","\n","    g_z = generator_model.predict(z)\n","\n","    gen_samples = pd.DataFrame(g_z, columns=data_cols)\n","    gen_samples.to_csv('Generated_sample.csv')\n","    plt.scatter( gen_samples[[col1]], gen_samples[[col2]], marker=markers[0], edgecolors=colors[0], facecolors='none' )\n","    plt.title(\"Generated Data\")   \n","    plt.xlabel(data_cols[0])\n","    ax.set_xlim(xlims), ax.set_ylim(ylims)\n","\n","plt.suptitle('Comparison of GAN outputs', size=16, fontweight='bold')\n","plt.tight_layout(rect=[0.075,0,1,0.95])\n","\n","# Adding text labels for traning steps\n","vpositions = np.array([ i._position.bounds[1] for i in axarr ])\n","vpositions += ((vpositions[0] - vpositions[1]) * 0.35 )\n","for model_step_ix, model_step in enumerate( model_steps ):\n","    fig.text( 0.05, vpositions[model_step_ix], 'training\\nstep\\n'+str(model_step), ha='center', va='center', size=12)\n","\n","plt.savefig('Comparison_of_GAN_outputs.png')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T04:21:08.462315Z","iopub.status.busy":"2022-06-10T04:21:08.461979Z","iopub.status.idle":"2022-06-10T04:21:08.488054Z","shell.execute_reply":"2022-06-10T04:21:08.486922Z","shell.execute_reply.started":"2022-06-10T04:21:08.462278Z"},"trusted":true},"outputs":[],"source":["g_z=pw.inverse_transform(g_z)\n","gen_samples = pd.DataFrame(g_z, columns=data_cols)\n","gen_samples.to_csv('Generated_sample.csv')"]},{"cell_type":"markdown","metadata":{},"source":["Now let's try to do a feature by feature comparision between the generated data and the actual data. We will use python's table_evaluator library to compare the features."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T04:21:31.712238Z","iopub.status.busy":"2022-06-10T04:21:31.711772Z","iopub.status.idle":"2022-06-10T04:21:47.684161Z","shell.execute_reply":"2022-06-10T04:21:47.683221Z","shell.execute_reply.started":"2022-06-10T04:21:31.712184Z"},"trusted":true},"outputs":[],"source":["!pip install table_evaluator"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T04:21:18.457621Z","iopub.status.busy":"2022-06-10T04:21:18.457298Z","iopub.status.idle":"2022-06-10T04:21:18.464999Z","shell.execute_reply":"2022-06-10T04:21:18.463996Z","shell.execute_reply.started":"2022-06-10T04:21:18.457588Z"},"trusted":true},"outputs":[],"source":["\n","print(gen_samples.columns)\n","print(df.shape, gen_samples.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T04:21:57.263505Z","iopub.status.busy":"2022-06-10T04:21:57.262664Z","iopub.status.idle":"2022-06-10T04:21:57.272358Z","shell.execute_reply":"2022-06-10T04:21:57.271508Z","shell.execute_reply.started":"2022-06-10T04:21:57.263449Z"},"trusted":true},"outputs":[],"source":["from table_evaluator import TableEvaluator\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-10T04:21:58.947126Z","iopub.status.busy":"2022-06-10T04:21:58.946065Z","iopub.status.idle":"2022-06-10T04:22:04.682064Z","shell.execute_reply":"2022-06-10T04:22:04.680280Z","shell.execute_reply.started":"2022-06-10T04:21:58.947063Z"},"trusted":true},"outputs":[],"source":["\n","print(len(df), len(gen_samples))\n","table_evaluator =  TableEvaluator(df, gen_samples)\n","\n","table_evaluator.visual_evaluation()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
